{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coding-linear-regression-with-pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EBdO9X_wjmmA"
      },
      "outputs": [],
      "source": [
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will input values of temperature, rainfall and humidity taking as random values. Tensor takes inputs in the form of tensors. The columns in each row denote values of temperature, rainfall and humidity. We have given the input datatypes as float and 32 denotes the number of bit given to these tensors."
      ],
      "metadata": {
        "id": "3y1UqDLwkEzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=torch.tensor([[73, 67, 43],\n",
        "                      [91, 88, 64],\n",
        "                      [87, 134, 58],\n",
        "                      [102, 43, 37],\n",
        "                       [69, 96, 70]], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "S7IAFz-bj2BL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will have outputs as yields of oranges and mangoes."
      ],
      "metadata": {
        "id": "m7VJVefIkI-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targets=torch.tensor([[56, 70],\n",
        "                       [81, 101],\n",
        "                       [119, 133],\n",
        "                       [22, 37],\n",
        "                       [103, 119]],dtype=torch.float32)"
      ],
      "metadata": {
        "id": "4HVJQWmzkIBw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to generate random values for these parameters. Now in again for machine learning jargon the coefficients(c₁) are called as weights and the contant term c₀ is called as bias term . So we use torch.randn function creates a tensor with the given shape, with elements picked randomly from a normal distribution with mean 0 and standard deviation 1."
      ],
      "metadata": {
        "id": "iTJc_aK6kNbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation of Weights and biases\n",
        "w = torch.randn(2, 3, requires_grad=True)\n",
        "b = torch.randn(2, requires_grad=True)\n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3jEF9DdkUl4",
        "outputId": "7654be7c-e553-45f6-a4f0-91ea59765329"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.1128, -1.5336, -0.7399],\n",
            "        [ 0.4458,  0.2488,  0.2831]], requires_grad=True)\n",
            "tensor([-0.0365, -1.7325], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With PyTorch, we have the ability to calculate the gradient or derivative of the loss w.r.t. to the weights and biases because they have requires_grad set to True.\n",
        "@ represents matrix multiplication in PyTorch, and the .t method returns the transpose of a tensor. The matrix obtained by passing the input data into the model is a set of predictions for the target variables. Here create a function which gives the simple matrix multiplication of weights with the respective variable input values and added bias to it."
      ],
      "metadata": {
        "id": "NgXgOqFkkXAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x):\n",
        "     return x @ w.t() + b"
      ],
      "metadata": {
        "id": "mGDImM7ykZKo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can generate predictions as by giving inputs to the model."
      ],
      "metadata": {
        "id": "IqiXIaeBkcwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-ow3psykfhn",
        "outputId": "82ddb5ef-b538-4d7e-89eb-e70499ee8468"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-215.8400,   59.6534],\n",
            "        [-283.6144,   78.8475],\n",
            "        [-345.2708,   86.8115],\n",
            "        [-206.8653,   64.9110],\n",
            "        [-275.8408,   72.7293]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to compute the error now by defining the error function. We give inputs as predictions and targets to these function get the squared differences’ average as the output. .numel()is a tensor function which gives us the total number of inputs given. Hence squared difference sum divided by total number of inputs is our output."
      ],
      "metadata": {
        "id": "MsS2FDsSkkFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(t1,t2):\n",
        "  diff=t1-t2\n",
        "  return torch.sum(diff*diff)/diff.numel()"
      ],
      "metadata": {
        "id": "-nXQCyb5khr_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss=mse(targets,preds)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7fU543zlijZ",
        "outputId": "f2154533-5098-49e5-f831-2d465571167c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(62393.8828, grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now pytorch has given us the ability to calculate the gradients over the loss function which we will use now."
      ],
      "metadata": {
        "id": "DFxPd9EYkoUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the gradients here\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "Ok-6K5tqkqR3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to check the values of each of the gradients you can print them as follows"
      ],
      "metadata": {
        "id": "h6igg9BVks9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nx70muJksQ4",
        "outputId": "3746bd28-ee90-4118-fda3-6062a3bf0eb5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-28580.0137, -31744.3105, -19387.8047],\n",
            "        [ -1427.0693,  -2414.7451,  -1349.5691]])\n",
            "tensor([-341.6863,  -19.4095])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradients values will be unique to each one as the rand give random values. Also check the values of weights and bias to compare with the gradients."
      ],
      "metadata": {
        "id": "9d5yx2YIkyRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  w -= w.grad * 1e-5\n",
        "  b -= b.grad * 1e-5\n",
        "  w.grad.zero_()\n",
        "  b.grad.zero_()"
      ],
      "metadata": {
        "id": "TVlSFXZdk1-o"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So here we will use the pytorch function of torch.no_grad() to disable the calculation of gradients as it consumes a lot of memory. This is only required only when we have requires_grad=True in for these variables specified earlier. Also another important point is resetting the gradients to zero as pytorch keeps on accumulating the gradients. One must notice that we have multiplied the new weights and bias with the learning. Deciding the learning comes by use of trail and error method. Whichever value gives the faster reduction in values of error must be used\n",
        "Now we must run this algorithm for a number of cycles until the error is minimum. We have chosen 100 as the number here which can be varied according to dataset used"
      ],
      "metadata": {
        "id": "kvsyN3k5k9QH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for 100 epochs\n",
        "for i in range(100):\n",
        " preds = model(inputs)\n",
        " loss = mse(preds, targets)\n",
        " loss.backward()\n",
        " with torch.no_grad():\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()"
      ],
      "metadata": {
        "id": "k2tPsM5Wk_bH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we need to calculate the loss to check if we have really reduced the it. For this dataset any error in range of 100 is good enough as it is squared value."
      ],
      "metadata": {
        "id": "ekcP3Z_QlL6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate loss\n",
        "preds = model(inputs)\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBRUYG7WlOEi",
        "outputId": "dfc78eea-882b-4a78-aa77-3d0ea415f5f2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(207.8029, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One must check the closeness in values of predictions and targets ."
      ],
      "metadata": {
        "id": "aShk6GYLlP9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(preds)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoiQhcadlTKA",
        "outputId": "92ad6a3c-dfff-4301-8cb2-942faaf6a31f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 62.0590,  74.3972],\n",
            "        [ 84.0368, 100.5434],\n",
            "        [106.6636, 126.6689],\n",
            "        [ 49.3961,  62.1706],\n",
            "        [ 88.5765, 103.8958]], grad_fn=<AddBackward0>)\n",
            "tensor([[ 56.,  70.],\n",
            "        [ 81., 101.],\n",
            "        [119., 133.],\n",
            "        [ 22.,  37.],\n",
            "        [103., 119.]])\n"
          ]
        }
      ]
    }
  ]
}